from mage_ai.data_preparation.shared.secrets import get_secret_value
from sqlalchemy import create_engine, text
import pandas as pd

if 'data_exporter' not in globals():
    from mage_ai.data_preparation.decorators import data_exporter

def sanitize_for_postgres(df):
    """
    Sanitiza DataFrame para PostgreSQL convertendo dicts e listas para string
    """
    if df.empty:
        return df  # Retorna DataFrame vazio sem modifica√ß√µes
        
    for col in df.columns:
        if df[col].apply(lambda x: isinstance(x, (dict, list))).any():
            df[col] = df[col].apply(lambda x: str(x) if isinstance(x, (dict, list)) else x)
    return df

def upsert_dataframe(df, table_name, schema, engine, primary_key):
    """
    Fun√ß√£o para fazer upsert (INSERT ... ON CONFLICT DO UPDATE) no PostgreSQL
    Trata adequadamente DataFrames vazios
    """
    # Verifica se o DataFrame est√° vazio
    if df.empty:
        print(f"‚ÑπÔ∏è  DataFrame vazio para {table_name} - pulando upsert")
        return
    
    temp_table = f"{table_name}_temp"
    
    with engine.begin() as conn:
        # 1. Criar tabela tempor√°ria com os novos dados
        df.to_sql(
            name=temp_table,
            schema=schema,
            con=conn,
            if_exists='replace',
            index=False
        )
        
        # 2. Verificar se a tabela principal existe
        table_exists = conn.execute(text(f"""
            SELECT EXISTS (
                SELECT FROM information_schema.tables 
                WHERE table_schema = '{schema}' 
                AND table_name = '{table_name}'
            )
        """)).scalar()
        
        if not table_exists:
            # 3a. Criar tabela principal pela primeira vez (com constraint)
            columns = df.columns.tolist()
            
            # Definir tipos de colunas baseado no DataFrame
            column_definitions = []
            for col in columns:
                if col == primary_key:
                    if df[col].dtype == 'int64':
                        column_definitions.append(f'"{col}" BIGINT PRIMARY KEY')
                    else:
                        column_definitions.append(f'"{col}" TEXT PRIMARY KEY')
                else:
                    if df[col].dtype == 'int64':
                        column_definitions.append(f'"{col}" BIGINT')
                    elif df[col].dtype == 'float64':
                        column_definitions.append(f'"{col}" DOUBLE PRECISION')
                    elif df[col].dtype == 'bool':
                        column_definitions.append(f'"{col}" BOOLEAN')
                    else:
                        column_definitions.append(f'"{col}" TEXT')
            
            create_table_sql = f"""
                CREATE TABLE {schema}.{table_name} (
                    {', '.join(column_definitions)}
                )
            """
            conn.execute(text(create_table_sql))
            print(f"üìù Tabela {table_name} criada com PRIMARY KEY em '{primary_key}'")
        else:
            # 3b. Verificar se existe uma constraint de unicidade na coluna desejada
            constraint_exists = conn.execute(text(f"""
                SELECT EXISTS (
                    SELECT 1 FROM information_schema.table_constraints tc
                    JOIN information_schema.key_column_usage kcu 
                    ON tc.constraint_name = kcu.constraint_name
                    WHERE tc.table_schema = '{schema}' 
                    AND tc.table_name = '{table_name}'
                    AND kcu.column_name = '{primary_key}'
                    AND tc.constraint_type IN ('PRIMARY KEY', 'UNIQUE')
                )
            """)).scalar()
            
            if not constraint_exists:
                try:
                    # Verificar se j√° existe uma PRIMARY KEY na tabela
                    pk_exists = conn.execute(text(f"""
                        SELECT EXISTS (
                            SELECT FROM information_schema.table_constraints 
                            WHERE table_schema = '{schema}' 
                            AND table_name = '{table_name}'
                            AND constraint_type = 'PRIMARY KEY'
                        )
                    """)).scalar()
                    
                    if pk_exists:
                        # J√° existe PK, criar UNIQUE constraint
                        conn.execute(text(f"""
                            ALTER TABLE {schema}.{table_name} 
                            ADD CONSTRAINT uk_{table_name}_{primary_key} 
                            UNIQUE ("{primary_key}")
                        """))
                        print(f"üîë UNIQUE constraint adicionada na coluna '{primary_key}' (PRIMARY KEY j√° existe)")
                    else:
                        # N√£o existe PK, pode criar uma
                        conn.execute(text(f"""
                            ALTER TABLE {schema}.{table_name} 
                            ADD PRIMARY KEY ("{primary_key}")
                        """))
                        print(f"üîë PRIMARY KEY adicionada na coluna '{primary_key}'")
                        
                except Exception as e:
                    print(f"‚ö†Ô∏è N√£o foi poss√≠vel adicionar constraint: {e}")
                    # Como √∫ltima alternativa, tentar UNIQUE com nome diferente
                    try:
                        conn.execute(text(f"""
                            CREATE UNIQUE INDEX IF NOT EXISTS idx_unique_{table_name}_{primary_key}
                            ON {schema}.{table_name} ("{primary_key}")
                        """))
                        print(f"üîë UNIQUE INDEX criado na coluna '{primary_key}'")
                    except Exception as e2:
                        print(f"‚ùå Erro ao criar qualquer constraint: {e2}")
                        raise
            else:
                print(f"‚úÖ Constraint de unicidade j√° existe na coluna '{primary_key}'")
        
        # 4. Preparar colunas para o upsert
        columns = df.columns.tolist()
        columns_str = ', '.join([f'"{col}"' for col in columns])
        
        # Colunas para UPDATE (excluindo a chave prim√°ria)
        update_columns = [col for col in columns if col != primary_key]
        update_str = ', '.join([f'"{col}" = EXCLUDED."{col}"' for col in update_columns])
        
        # 5. Executar UPSERT
        upsert_query = f"""
        INSERT INTO {schema}.{table_name} ({columns_str})
        SELECT {columns_str} FROM {schema}.{temp_table}
        ON CONFLICT ("{primary_key}") 
        DO UPDATE SET {update_str}
        """
        
        conn.execute(text(upsert_query))
        
        # 6. Remover tabela tempor√°ria
        conn.execute(text(f"DROP TABLE {schema}.{temp_table}"))
        
        print(f"‚úÖ Upsert conclu√≠do para {table_name}: {len(df)} registros processados")

@data_exporter
def export_cartpanda_data(data, *args, **kwargs):
    """
    Exporta dados do CartPanda para PostgreSQL com tratamento robusto para casos sem dados
    Funciona tanto com dados vazios quanto com metadados de execu√ß√£o
    """
    
    # ETAPA 1: Verifica√ß√£o e tratamento dos dados de entrada
    print("üîÑ Iniciando exporta√ß√£o dos dados CartPanda...")
    
    # Caso 1: Dados com estrutura de metadados (vindos do transformer melhorado)
    if isinstance(data, dict) and 'execution_metadata' in data:
        print("üìã Recebidos dados com metadados de execu√ß√£o")
        
        # Verifica se h√° dados para processar
        if not data['execution_metadata']['has_data']:
            print(f"‚ÑπÔ∏è  {data['execution_metadata']['message']}")
            print("‚ú® Data exporter finalizado graciosamente - nenhum dado para exportar")
            print(f"üìÖ Execu√ß√£o registrada em: {data['execution_metadata']['extraction_date']}")
            return  # Finaliza a execu√ß√£o sem erro
        
        # Se chegou aqui, h√° dados nos metadados para processar
        df_orders = data.get('orders_df', pd.DataFrame())
        df_items = data.get('items_df', pd.DataFrame())
        print("üì¶ Processando dados dos metadados...")
    
    # Caso 2: Estrutura tradicional (dict com orders_df e items_df)
    elif isinstance(data, dict) and ('orders_df' in data or 'items_df' in data):
        df_orders = data.get('orders_df', pd.DataFrame())
        df_items = data.get('items_df', pd.DataFrame())
        print("üì¶ Processando dados da estrutura tradicional...")
    
    # Caso 3: Tipo de dado inesperado
    else:
        print(f"‚ö†Ô∏è  Tipo de dado inesperado recebido: {type(data)}")
        print("‚ùå N√£o foi poss√≠vel extrair DataFrames dos dados recebidos")
        print("‚ú® Data exporter finalizado - estrutura de dados n√£o reconhecida")
        return
    
    # ETAPA 2: Verifica√ß√£o se h√° dados para exportar
    orders_empty = df_orders.empty if isinstance(df_orders, pd.DataFrame) else True
    items_empty = df_items.empty if isinstance(df_items, pd.DataFrame) else True
    
    if orders_empty and items_empty:
        print("‚ÑπÔ∏è  Ambos DataFrames (orders e items) est√£o vazios")
        print("‚ú® Pipeline finalizado com sucesso - nenhum dado para exportar")
        return
    
    # ETAPA 3: Relat√≥rio dos dados que ser√£o exportados
    orders_count = len(df_orders) if not orders_empty else 0
    items_count = len(df_items) if not items_empty else 0
    
    print(f"\nüìä DADOS PARA EXPORTA√á√ÉO:")
    print(f"   ‚Ä¢ Pedidos: {orders_count} registros")
    print(f"   ‚Ä¢ Itens: {items_count} registros")
    
    # ETAPA 4: Sanitiza√ß√£o dos dados
    print("üßπ Sanitizando dados para PostgreSQL...")
    
    try:
        df_orders_clean = sanitize_for_postgres(df_orders) if not orders_empty else pd.DataFrame()
        df_items_clean = sanitize_for_postgres(df_items) if not items_empty else pd.DataFrame()
        print("‚úÖ Sanitiza√ß√£o conclu√≠da")
    except Exception as e:
        print(f"‚ùå Erro durante sanitiza√ß√£o: {e}")
        print("‚ú® Data exporter finalizado devido a erro na sanitiza√ß√£o")
        return

    # ETAPA 5: Exporta√ß√£o para DATA LAKE RAILWAY
    print("\nüöÇ Iniciando exporta√ß√£o para DATA LAKE RAILWAY...")
    
    try:
        POSTGRES_HOST = get_secret_value('POSTGRES_HOST_RAILWAY')
        POSTGRES_PORT = get_secret_value('POSTGRES_PORT_RAILWAY')
        POSTGRES_DB   = get_secret_value('POSTGRES_DB_RAILWAY')
        POSTGRES_USER = get_secret_value('POSTGRES_USER_RAILWAY')
        POSTGRES_PASS = get_secret_value('POSTGRES_PASS_RAILWAY')

        datalake_railway_conn_string = (
            f'postgresql+psycopg2://{POSTGRES_USER}:{POSTGRES_PASS}'
            f'@{POSTGRES_HOST}:{POSTGRES_PORT}/{POSTGRES_DB}'
        )
        engine_railway = create_engine(datalake_railway_conn_string)
        
        # Garante a exist√™ncia do schema
        with engine_railway.begin() as conn:
            conn.execute(text("CREATE SCHEMA IF NOT EXISTS integracao"))
        
        # Exporta pedidos se houver dados
        if not orders_empty:
            upsert_dataframe(
                df=df_orders_clean,
                table_name='cartpanda_orders',
                schema='integracao',
                engine=engine_railway,
                primary_key='id'
            )
        else:
            print("‚ÑπÔ∏è  Pulando exporta√ß√£o de pedidos (DataFrame vazio)")
        
        # Exporta itens se houver dados
        if not items_empty:
            upsert_dataframe(
                df=df_items_clean,
                table_name='cartpanda_items',
                schema='integracao',
                engine=engine_railway,
                primary_key='item_id'
            )
        else:
            print("‚ÑπÔ∏è  Pulando exporta√ß√£o de itens (DataFrame vazio)")
        
        print('‚úÖ Exporta√ß√£o para DATA LAKE RAILWAY conclu√≠da')
        
    except Exception as e:
        print(f"‚ùå Erro na exporta√ß√£o para Railway: {e}")
        print("‚ö†Ô∏è Continuando com exporta√ß√£o para banco de replica√ß√£o...")

    # ETAPA 6: Exporta√ß√£o para REPLICA√á√ÉO/DEV (VPS)
    print("\nüîÑ Iniciando exporta√ß√£o para REPLICA√á√ÉO/DEV...")
    
    try:
        POSTGRES_HOST = get_secret_value('POSTGRES_HOST')
        POSTGRES_PORT = get_secret_value('DB_PORT')
        POSTGRES_DB   = get_secret_value('DB_NAME')
        POSTGRES_USER = get_secret_value('DB_USER')
        POSTGRES_PASS = get_secret_value('DB_PASSWORD')

        connection_string = (
            f'postgresql+psycopg2://{POSTGRES_USER}:{POSTGRES_PASS}'
            f'@{POSTGRES_HOST}:{POSTGRES_PORT}/{POSTGRES_DB}'
        )

        engine_rep = create_engine(connection_string)

        # Garante a exist√™ncia do schema "integracao"
        with engine_rep.begin() as conn:
            conn.execute(text("CREATE SCHEMA IF NOT EXISTS integracao"))

        # Exporta pedidos se houver dados
        if not orders_empty:
            upsert_dataframe(
                df=df_orders_clean,
                table_name='cartpanda_orders',
                schema='integracao',
                engine=engine_rep,
                primary_key='id'
            )
        else:
            print("‚ÑπÔ∏è  Pulando exporta√ß√£o de pedidos (DataFrame vazio)")

        # Exporta itens se houver dados
        if not items_empty:
            upsert_dataframe(
                df=df_items_clean,
                table_name='cartpanda_items',
                schema='integracao',
                engine=engine_rep,
                primary_key='item_id'
            )
        else:
            print("‚ÑπÔ∏è  Pulando exporta√ß√£o de itens (DataFrame vazio)")
        
        print('‚úÖ Exporta√ß√£o para REPLICA√á√ÉO/DEV conclu√≠da')
        
    except Exception as e:
        print(f"‚ùå Erro na exporta√ß√£o para Replica√ß√£o: {e}")
        raise

    # ETAPA 7: Relat√≥rio final
    print(f"\nüéâ EXPORTA√á√ÉO CONCLU√çDA COM SUCESSO!")
    print(f"   ‚Ä¢ Pedidos exportados: {orders_count}")
    print(f"   ‚Ä¢ Itens exportados: {items_count}")
    print(f"   ‚Ä¢ Destinos: Railway Data Lake + Replica√ß√£o VPS Hostinger")
    print(f"   ‚Ä¢ Schema: integracao")
    print(f"   ‚Ä¢ Opera√ß√£o: UPSERT (sem duplicatas)")